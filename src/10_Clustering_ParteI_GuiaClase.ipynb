{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RuCdK0K1ekunX7a9NzAorIG6aH-KoBpN","timestamp":1716928565636}],"authorship_tag":"ABX9TyPffYPUGq7mSlVpSNmJ5NkV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Practica 3: Clustering - Parte I\n","\n","En esta primera parte, vamos a utilizar los algoritmos de Kmeans, aglomerativo, DBSCAN y Spectral Clustering en un conjunto de datos de juguete, para analizar las ventajas y desventajas de cada uno de los métodos, efectos de inicialización y selección de parámetros"],"metadata":{"id":"2FLMLMpQ1hG6"}},{"cell_type":"markdown","source":["Vamos a generar un dataset de juguete empleando las herramientas de sklearn:\n","\n","*   make_blobs: genera nubes de puntos gaussiana. Por defecto, generar 3 nubes.   \n","*   make_circles: genera un conjunto de datos distribuidos en dos circulos (uno dentro del otro) en dos dimensiones\n","*   make_moons: genera un conjunto de datos distirbuidos en dos semicirculos\n","\n"],"metadata":{"id":"qSpctmSD2VmE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPpE6YTnrcdC"},"outputs":[],"source":["#Importar librerias\n"]},{"cell_type":"code","source":["#Generamos 500 muestras para cada dataset\n","n_samples = 500\n","seed = 30\n","#Nubes de puntos gaussianos (3 clusters)\n","\n","#Circulos concentricos (2 clusters)\n","\n","#Semicirculos (2 clusters)\n","\n","#Nubes de puntos gaussianos (10 clusters)\n"],"metadata":{"id":"JZiY9XDzr5bM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Grafica de los datasets\n","scaler = StandardScaler()\n","colors = np.array([\"#377eb8\",\n","                   \"#ff7f00\",\n","                   \"#4daf4a\",\n","                   \"#f781bf\",\n","                   \"#a65628\",\n","                   \"#984ea3\",\n","                   \"#999999\",\n","                   \"#e41a1c\",\n","                   \"#dede00\",\n","                   \"#008000\",\n","                   \"#0343DF\",\n","                   \"#7FFF00\",\n","                   \"#ED0DD9\",\n","                   \"#FBDD7E\",\n","                   \"#FFA500\"])\n","plt.figure(figsize=(15, 5))\n","\n","data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n","i=1\n","for dataset in data_sets:\n","  X = scaler.fit_transform(dataset[0])\n","  y = dataset[1]\n","  plt.subplot(1,len(data_sets),i)\n","  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n","  i+=1"],"metadata":{"id":"FohseQRft8_k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##K-means\n","El término «k-means» fue utilizado por primera vez por James MacQueen en 1967, aunque la idea se remonta a Hugo Steinhaus en 1956. El algoritmo estándar fue propuesto por primera vez por Stuart Lloyd, de los Laboratorios Bell, en 1957 como técnica para la modulación por impulsos codificados, aunque no se publicó como artículo en una revista hasta 1982.\n","\n","Es el algoritmo más popular y empleado de todos los metodos de clustering. Fácil de implementar, computacionalmente rapido, escalable, que permite obtener sub-cluster cambiando el número de cluster. Solo trabaja para nubes de puntos convexas y depende del valor de k.\n","\n","Se pueden encontrar aplicaciones en geoestadistica, visión por computador, segmentación de mercado, estudios de terremotos, uso de suelo, entre otros."],"metadata":{"id":"3QBQe4TvvoBQ"}},{"cell_type":"code","source":["#Importar Kmeans\n"],"metadata":{"id":"l0524EfbvqHg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero vamos aplicarlo al dataset de nube de puntos con tres clusters. En esta primera parte del experimento vamos a asumir que conocemos el número de clusters."],"metadata":{"id":"FgU6HC3U7TiP"}},{"cell_type":"code","source":["#Organizamos los datos\n","\n","#Y: Solo por referencia, no se utilizan en el algoritmo\n","\n","#Configuramos los parametros\n","#Ingresamos número de clusters 'n_clusters'\n","#Seleccionamos la inicialización aleatoria\n","#Definimos cuantas veces se repite el proceso n_init = 10 por defecto\n","\n","#Entrenamos el modelo\n","\n","#Obtenemos las etiquetas de pertenencia\n","\n","#Graficamos el resultado\n"],"metadata":{"id":"TqnwyedBv5X5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Una forma de evaluar el resultado es mediante la suma de las distancias al cuadrado de cada muestra al centroide de su cluster, este parametro se calcula durante el entrenamiento, y queda almacenado en el atributo inertia_.\n","\n"],"metadata":{"id":"p3ApOXNhFZxn"}},{"cell_type":"code","source":["#Mostrar la suma de la distancia al cuadrado al centroide\n"],"metadata":{"id":"04sf1jTaFWVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora vamos aplicar Kmeans a todos los conjuntos de datos!!!"],"metadata":{"id":"TjEF2VSl8XMK"}},{"cell_type":"code","source":["#Organizamos los datos\n","\n","#El número de clusters a buscar en cada conjunto\n","\n","#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n","for i, dataset in enumerate(data_sets):\n","  X = scaler.fit_transform(dataset[0])\n","  y = dataset[1]\n","  #Graficamos los datos originales\n","\n","\n","  #Configuramos KMeans\n","\n","  #Entrenamos el modelo\n","\n","  #Extraemos las etiquetas y la suma de la distancias\n","\n","\n","  #Graficamos los clusters finales\n"],"metadata":{"id":"OSKL9akM8cNR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nota importante: los colores solo permiten establecer la correspondencia a un mismo clusters, más no permite comparar entre los datos originales y los obtenidos con el algoritmo."],"metadata":{"id":"MkqEpO8K-gM2"}},{"cell_type":"code","source":["#Mostramos la suma de distancias al cuadrado a los centroides"],"metadata":{"id":"xzaPlscb7-9E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Metodos Jerarquico Aglomerativo\n","\n","Producen una serie anidada de particiones. La representación de los resultados se realiza a través de un dendrograma, que representa la agrupación anidada de patrones y los niveles de similaridad. Aunque no requiere definir el número de clusters, se necesita determinar donde partir el dendrograma para obtener los clusters finales. Los métodos aglomerativo se caracterizan por partir con un cluster por cada muestra, e ir fusionando los clusters por su similaridad hasta obtener el numero deseado.\n","\n","Son algoritmos conceptualmente simples, con buen desempeño para pequeños conjuntos de datos. Aunque presenta dificultades cuando debe manejar diferentes formas de cluster no convexas.\n","\n","Estos algoritmos tienen aplicaciones en reconocimiento de patrones, segmentación de imágenes, redes de sensores wireless, planeación de ciudades, y analisis de datos espaciales, entre otros.\n"],"metadata":{"id":"5AQ9iLvggK8a"}},{"cell_type":"code","source":["#Importar libreria\n"],"metadata":{"id":"p8Ch284LhOvR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero vamos aplicarlo al dataset de nube de puntos con tres clusters, asumiendo el número de clusters. Inicialmente usaremos la distancia euclidiana con el enlaze simple.\n","\n","Existen diferentes tipos de enlace con los cuales se puede experimentar:\n","\n","\n","*   Simple: usa la distancia minima entre un par de observaciones de dos clusters\n","*   Completo: usa la distancia maxima entre par de observaciones de dos clusters\n","*   Promedio: usar la distancia promedio entre pares de observaciones de dos clusters\n","*   Ward: minimiza la varianza de los clusters que van a ser fusionados\n","\n"],"metadata":{"id":"1QUuGxebhV7I"}},{"cell_type":"code","source":["#Organizamos los datos\n","X = scaler.fit_transform(blobs[0])\n","y = blobs[1] #Solo por referencia, no se utilizan en el algoritmo\n","\n","#Configuramos los parametros\n","#Ingresamos número de clusters 'n_clusters'\n","#Usamos euclidina y enlace simple\n","\n","#Entrenamos el modelo\n","\n","#Obtenemos las etiquetas de pertenencia\n","\n","#Graficamos el resultado\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_agglo])"],"metadata":{"id":"lVT8IXq3iKXL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comparemos que ocurre con otros enlaces.\n","Nota: para el enlace ward solo acepta la distancia euclidiana."],"metadata":{"id":"_4WiUgRYjjAa"}},{"cell_type":"code","source":["#Enlace completo\n","####\n","clustering.fit(X)\n","y_pred_aggloC = clustering.labels_\n","\n","#Enlace promedio\n","####\n","clustering.fit(X)\n","y_pred_aggloA = clustering.labels_\n","\n","#Enlace Ward\n","####\n","clustering.fit(X)\n","y_pred_aggloW = clustering.labels_\n","\n","#Graficamos los clusters finales\n","plt.figure(figsize=(15, 5))\n","plt.subplot(1,4,1)\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_agglo])\n","plt.title('Single Linkage', size=18)\n","plt.subplot(1,4,2)\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_aggloC])\n","plt.title('Complete Linkage', size=18)\n","plt.subplot(1,4,3)\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_aggloA])\n","plt.title('Average Linkage', size=18)\n","plt.subplot(1,4,4)\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_aggloW])\n","plt.title('Ward Linkage', size=18)"],"metadata":{"id":"kI_x-CkRjfxQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos aplicar el metodo aglomerativo a nuestro cuatros conjuntos de datos!!\n"],"metadata":{"id":"fwpMbZskk68l"}},{"cell_type":"code","source":["#Organizamos los datos\n","data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n","type_linkage = ['single', 'complete', 'average', 'ward']\n","#El número de clusters a buscar en cada conjunto\n","data_n = [3,2,2,10]\n","plt.figure(figsize=(15, 10))\n","#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n","for i, dataset in enumerate(data_sets):\n","  X = scaler.fit_transform(dataset[0])\n","  y = dataset[1]\n","  #Graficamos los datos originales\n","  plt.subplot(5,len(data_sets),i+1)\n","  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n","  for j, link in enumerate(type_linkage):\n","    #Configuramos, entrenamos y extraemos etiquetas\n","    #####\n","    #Graficamos los clusters finales\n","    plt.subplot(5,len(data_sets),i+5+j*4)\n","    plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_CA])\n","    plt.title(link, size=12)"],"metadata":{"id":"ETDnUwd1k6W5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DBSCAN\n","El algoritmo basado en densidad espacial para aplicaciones con ruido (DBSCAN por sus siglas en ingles) fue propuesto por Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.\n","\n","Es un algoritmo que puede diferenciar diferentes formas (incluso no convexas) y tamaños. Puede encontrar clusters rodeados completamente por otros clusters, y es robusto frente a la presencia de ruido y datos anomalos.\n","\n","Se emplea en aplicaciones de revisión de literatura cientifica, imágenes satelitales, cristalografía de rayos x, y detección de anomalias."],"metadata":{"id":"gZAis3zev4YN"}},{"cell_type":"code","source":["#Importar libreria\n"],"metadata":{"id":"f27F47Lswon6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero vamos aplicarlo al dataset de nube de puntos con tres clusters. Inicialmente usaremos la distancia euclidiana y los parametros por defecto:\n","\n","\n","*   eps = 0.5\n","*   min_samples = 5\n","\n"],"metadata":{"id":"A8Ln2bENwBje"}},{"cell_type":"code","source":["#Organizamos los datos\n","X = scaler.fit_transform(blobs[0])\n","y = blobs[1] #Solo por referencia, no se utilizan en el algoritmo\n","\n","#Configuramos los parametros\n","#Usamos euclidina\n","\n","#Entrenamos el modelo\n","\n","#Obtenemos las etiquetas de pertenencia\n","\n","#Graficamos el resultado\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_db])\n","#Numero de clusters - eliminando noise points\n","n_clusters_ = len(set(y_pred_db)) - (1 if -1 in y_pred_db else 0)\n","n_noise_ = list(y_pred_db).count(-1)\n","print(\"Estimated number of clusters: %d\" % n_clusters_)\n","print(\"Estimated number of noise points: %d\" % n_noise_)"],"metadata":{"id":"ETxnHz4DxDNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Este algoritmo es muy sensible a los parametros eps y min_samples:\n"],"metadata":{"id":"jIZSo1u_xZ1P"}},{"cell_type":"code","source":["#Configuramos los parametros\n","\n","#Entrenamos el modelo\n","clustering.fit(X)\n","#Obtenemos las etiquetas de pertenencia\n","y_pred_db = clustering.labels_\n","#Graficamos el resultado\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_db])\n","#Numero de clusters - eliminando noise points\n","n_clusters_ = len(set(y_pred_db)) - (1 if -1 in y_pred_db else 0)\n","n_noise_ = list(y_pred_db).count(-1)\n","print(\"Estimated number of clusters: %d\" % n_clusters_)\n","print(\"Estimated number of noise points: %d\" % n_noise_)"],"metadata":{"id":"nHKeN-jwxZnf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos aplicar el metodo DBSCAN a nuestro cuatros conjuntos de datos!!\n"],"metadata":{"id":"mKtlLR4N0DdO"}},{"cell_type":"code","source":["#Organizamos los datos\n","data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n","sum_dist = []\n","#El número de clusters a buscar en cada conjunto\n","data_n = [3,2,2,10]\n","plt.figure(figsize=(15, 10))\n","#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n","for i, dataset in enumerate(data_sets):\n","  X = scaler.fit_transform(dataset[0])\n","  y = dataset[1]\n","  #Graficamos los datos originales\n","  plt.subplot(2,len(data_sets),i+1)\n","  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n","  #Configuramos DBSCAN\n","  ###\n","  #Entrenamos el modelo\n","  clustering.fit(X)\n","  #Extraemos las etiquetas\n","  y_pred_DB = clustering.labels_\n","  #Graficamos los clusters finales\n","  plt.subplot(2,len(data_sets),i+5)\n","  plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_DB])"],"metadata":{"id":"8RyVTTDX0DEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spectral Clustering\n","Hace uso de el espectro (valores propios) de la matriz de similaridad para reducir las dimensiones de los datos y realizar el proceso de agrupamiento.\n","\n","A parti de una matriz de similaridad, que representa los datos como un grafo, se define el laplaciano. A la matriz laplaciana se le encuentra los k vectores propios mas significativos y se le aplica el algoritmo de kmeans para agrupar los datos.\n","\n","Existen diferentes definiciones del Laplaciano. Entre las aplicaciones que se destacan esta la segmentación de imagenes."],"metadata":{"id":"kIBJm2zm3ll0"}},{"cell_type":"code","source":["#Importar libreria\n"],"metadata":{"id":"ufqwxQoi4Pb4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero vamos aplicarlo al dataset de nube de puntos con tres clusters. Entre los parametros a incluir se encuentran:\n","\n","\n","*   n_clusters: obligatorio para este algoritmo\n","*   n_init: numero de veces que se corre Kmeans con diferentes semillas, por defecto es 10\n","*   affinity: construcción de la matriz de afinidad, por defecto rbf, pero tambien puede usarse 'nearest_neighbors'\n","\n","\n"],"metadata":{"id":"eR8wtZP24gnI"}},{"cell_type":"code","source":["#Organizamos los datos\n","X = scaler.fit_transform(blobs[0])\n","y = blobs[1] #Solo por referencia, no se utilizan en el algoritmo\n","\n","#Configuramos los parametros\n","\n","#Entrenamos el modelo\n","\n","#Obtenemos las etiquetas de pertenencia\n","\n","#Graficamos el resultado\n","plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_SC])"],"metadata":{"id":"FVoNUzIn5HPY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos aplicar el metodo Spectral Clustering a nuestro cuatros conjuntos de datos!!"],"metadata":{"id":"Gcpsc8E85u2T"}},{"cell_type":"code","source":["#Organizamos los datos\n","data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n","sum_dist = []\n","#El número de clusters a buscar en cada conjunto\n","data_n = [3,2,2,10]\n","plt.figure(figsize=(15, 10))\n","#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n","for i, dataset in enumerate(data_sets):\n","  X = scaler.fit_transform(dataset[0])\n","  y = dataset[1]\n","  #Graficamos los datos originales\n","  plt.subplot(2,len(data_sets),i+1)\n","  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n","  #Configuramos Spectral Clustering\n","  ####\n","  #Entrenamos el modelo\n","  clustering.fit(X)\n","  #Extraemos las etiquetas\n","  y_pred_SC = clustering.labels_\n","  #Graficamos los clusters finales\n","  plt.subplot(2,len(data_sets),i+5)\n","  plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_SC])"],"metadata":{"id":"30Ym3DH25uWt"},"execution_count":null,"outputs":[]}]}